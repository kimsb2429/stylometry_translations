---
title: "Stylistic Comparison Between Fiction Written in English and Fiction Translated into English"
author: "Jae Kim"
output:
  html_document: default
  pdf_document: default
---
<style type="text/css">
h1.title {
  font-size: 24px;
  font-family: serif;
  text-align:center;
}
.author {
  font-size: 24px;
  font-family: serif;
  text-align:center;
  padding-bottom:30px;
}
h2 {
  font-size: 24px;
  padding-top:10px;
  font-family:serif;
  font-style:bold;
}
body {
  font-family:serif;
}

</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE, fig.align = 'center')
```



## Introduction

The motivation for this study comes from a 2011 study by James M. Hughes and others called "Quantitative patterns of stylistic influence in the evolution of literature." The study examined whether there was any evidence to support the notion of a literary "style of time" and whether authors writing in the same time period were more similar in style than authors writing in different time periods. This study will explore a variation to the Hughes study and ask whether there is a significant difference in the styles of literature written in English (which I will sometimes refer to as non-translated literature) and literature translated into English (which I will sometimes refer to as translated literature). 

For example, there are students in creative writing classes who complain, as soon as they learn that a short story has been translated from another language, that the story "sounds weird." But can a reader really tell whether they're reading a translated text? It doesn't seem impossible that this might indeed be the case. Perhaps translators are less stylistically concerned than original authors, and translations start to sound similar to one another. This study attempts to examine whether such a similarity really exists.

## Data

The data comes from Project Gutenberg. It has its limitations, as I will discuss in the concluding section, but it is one of the only repositories of literature that are available for free. Since Hughes et al. have also used Project Gutenberg, the results of this study can easily be compared to theirs. 

Since the Project Gutenberg r package doesn't identify translators as such, I've manually scraped their web page that contains the list of authors, where they also list the translators. I gathered the set of works that are labeled "(English) (as Translator)". This is the set of translated texts. I then subtracted this set from the full set of books whose language is labeled as English, which can be obtained via the r package. The remainder is the set of non-translated texts. I further filtered each set so that only those works considered to be fiction remain in the sets. I found there are **787** translated works of fiction and **12454** non-translated works of fiction.

## Methods

As Hughes et al. have done in their study, I used content-free words, and I borrow from their list of 307 such words that are considered devoid of content. They include, but are not limited to, prepositions, conjunctions, pronouns, and articles. As a point of illustration, below is a histogram containing the frequency of occurrence of ten content-free words in four novels by H.G. Wells (*The Time Machine*, *The War of the Worlds*, *The Invisible Man*, and *The Island of Doctor Moreau*) and in five novels by the Bronte sisters (*Jane Eyre*, *Wuthering Heights*, *The Tenant of Wildfell Hall*, *Villette*, and *Agnes Grey*).


```{r echo=FALSE, eval=FALSE}
str_example = "about above across after afterwards again against almost alone"
c <- strsplit(str_example," ")
```
```{r echo=FALSE, eval=FALSE}
g <- c(1:length(c[[1]]))
h <- c(1:length(c[[1]]))

for (i in 1:length(c[[1]])){
  key=toString(c[[1]][i])
  g[i] <- key
}

library(gutenbergr)
hgwells <- gutenberg_download(c(35, 36, 5230, 159))
library(tidyverse)
library(tidytext)
tidy_hgwells <- hgwells %>%
  group_by(gutenberg_id) %>%
  mutate(linenumber = row_number()) %>%
  ungroup() %>%
  unnest_tokens(word, text)

for (i in 1:length(g)){
  val = g[i]
  x = tidy_hgwells$word %>% 
      str_extract_all(val) %>%
      unlist %>%
      length
  h[i]=x
}
#h = h/sum(h) #Normalize to sum to 1
#h = h * 1000
k1 <- data.frame(g,h)
#ggplot(data=k1,aes(x=g,y=h))+geom_bar(stat="identity") + xlab("content-free words") + ylab("average # of occurrences per 1,000 words")
```
```{r echo=FALSE, eval=FALSE}
#g <- c(1:length(c[[1]]))
h <- c(1:length(c[[1]]))

#for (i in 1:length(c[[1]])){
#  key=toString(c[[1]][i])
#  g[i] <- key
#}

library(gutenbergr)
bronte <- gutenberg_download(c(1260, 768, 969, 9182, 767))
library(tidyverse)
library(tidytext)
tidy_bronte <- bronte %>%
  group_by(gutenberg_id) %>%
  mutate(linenumber = row_number()) %>%
  ungroup() %>%
  unnest_tokens(word, text)

for (i in 1:length(g)){
  val = g[i]
  x = tidy_bronte$word %>% 
      str_extract_all(val) %>%
      unlist %>%
      length
  h[i]=x
}
#h = h/sum(h) #normalized to sum to 1
#h = h * 1000
k2 <- data.frame(g,h)
#ggplot(data=k2,aes(x=g,y=h))+geom_bar(stat="identity") + xlab("content-free words") + ylab("average # of occurrences per 1,000 words")
```
```{r echo=FALSE, eval=FALSE}
bronte_sisters <- k2$h
hg_wells <- k1$h

df1 <- data.frame(hg_wells, bronte_sisters, g)
write.table(df1,file="wells_bronte_graph2.txt")
```
```{r echo=FALSE}
df_read <- read.table("wells_bronte_graph2.txt")
library(ggplot2)
library(reshape2)
df2 <- melt(df_read, id.vars='g')
ggplot(df2, aes(x=g, y=value, fill=variable)) +
    geom_bar(stat='identity', position='dodge') + xlab("content-free words") + ylab("frequency")
```

<br/>
Using the distribution of frequencies of the content-free words, one can visualize the stylistic difference among all the authors. The chart below, for example, arranges the authors in terms of their stylistic differences, using a method called Multi-Dimensional Scaling (MDS), treating each content-free word as a different factor that determines the authors' difference from one another. What multidimensional scaling allows one to do is to collapse the many dimensions in which these authors stand apart from one another onto, in this case, two dimensions, so that we can have a visual representation. I've also included the corresponding table with content-free words and counts. I'm only using the ten content-free words I've used above, and I have selected only those authors born after 1927, so that, for the sake of visualization, the number of represented authors is reasonably small.
<br/>



```{r echo=FALSE, eval=FALSE}
count = 1
authors <- subset(gutenberg_authors, !is.na(birthdate) & !is.na(deathdate) & birthdate > 1927)
authors_list <- authors$author  
y <- vector()
y2 <- vector()
z <- gutenberg_works(languages="en")
for (x in authors_list){
  if (x != "Garrett, Randall"){
  s <- subset(z,author== x & gutenberg_id > 0)
  if(length(s$gutenberg_id)>4){
    y[count] <- x
    y2 <- c(y2, list(s$gutenberg_id))
    count = count + 1
  }
  }
}
```
```{r echo=FALSE, include=TRUE, message=FALSE, warning=FALSE, eval=FALSE}
l = length(y2)
nx <- vector()
for (w in 1:l){
  n <- gutenberg_download(y2[[w]])
  nx <- c(nx, list(n))
}
mx1.length = length(y2) * length(y2)
mx1 <- matrix(rep(0,mx1.length), nrow=length(y2), ncol=length(y2))
rownames(mx1) <-y
colnames(mx1) <-y
for (w in 1:(l-1)){
  for (q in 1:(l-w)){
    q2 = w + q
    n <- nx[w][[1]]
    tidy_n <- n %>%
      group_by(gutenberg_id) %>%
      mutate(linenumber = row_number()) %>%
      ungroup() %>%
      unnest_tokens(word, text)
    m <- nx[q2][[1]]
    tidy_m <- m %>%
      group_by(gutenberg_id) %>%
      mutate(linenumber = row_number()) %>%
      ungroup() %>%
      unnest_tokens(word, text)
    DKL <- DKL_Distance(tidy_n,tidy_m,str)
    mx1[y[w],y[q2]]=DKL
    mx1[y[q2],y[w]]=DKL
  }
}
write.table(mx1,file="mx1.txt")
d <- dist(mx1) # euclidean distances between the rows
fit <- cmdscale(d,eig=TRUE, k=2) # k is the number of dim
# plot solution
x1 <- fit$points[,1]
y1 <- fit$points[,2]
#dev.new(width=20, height=16)
dfxy <- data.frame(x1,y1)
write.table(dfxy,file="mds1.txt")
```
```{r echo=FALSE, eval=FALSE}
library(ggplot2)
library(ggrepel)
dfxy <- read.table("mds1.txt")
mx1 <- read.table("mx1.txt")
ggplot(dfxy, width=3,height=10, aes(x=x1, y=y1, label= rownames(mx1), xlab="Coordinate 1", ylab="Coordinate 2", main="Metric MDS")
)+ geom_point(color = "red") + geom_text_repel() + labs(title = "MDS Representation") +
  theme(axis.title.x=element_blank(),
    axis.text.x=element_blank(),
    axis.ticks.x=element_blank(),
    axis.title.y=element_blank(),
    axis.text.y=element_blank(),
    axis.ticks.y=element_blank())
```





```{r echo=FALSE, eval=FALSE}
library(huxtable)
ht <- as_hux(mx1, add_colnames = TRUE, add_rownames = TRUE)

bottom_border(ht)[1,]  <- 0.4
left_border(ht)[,2]    <- 0.4
align(ht)[,2]          <- 'right'
right_padding(ht)      <- 10
left_padding(ht)       <- 10
width(ht)              <- 1
number_format(ht)      <- 3

ht %>% 
      set_all_padding(4) %>% 
      set_outer_padding(0) %>% 
      set_number_format(2) %>% 
      set_width(0.4)
      #set_caption("&lt;b&gt;Jam Prices&lt;/b&gt;")
```





```{r echo=FALSE, include=TRUE, message=FALSE, warning=FALSE, eval=FALSE}
mx.length = length(y2) * length(g)
mx <- matrix(1:mx.length, nrow=length(y2), ncol=length(g))
rownames(mx) <-y
colnames(mx) <-g
library(magrittr)
library(tidyverse)
library(tidytext)
for(w in 1:l){
n <- nx[w][[1]]
tidy_n <- n %>%
  group_by(gutenberg_id) %>%
  mutate(linenumber = row_number()) %>%
  ungroup() %>%
  unnest_tokens(word, text)

for (i in 1:length(g)){
  val = g[i]
  x = tidy_n$word %>% 
      str_extract_all(val) %>%
      unlist %>%
      length
  mx[y[w],g[i]]=x
}
}
write.table(mx,"mx.txt")
d <- dist(mx) # euclidean distances between the rows
fit <- cmdscale(d,eig=TRUE, k=2) # k is the number of dim
# plot solution
x1 <- fit$points[,1]
y1 <- fit$points[,2]
#dev.new(width=20, height=16)
dfxy <- data.frame(x1,y1)
write.table(dfxy,file="mds2.txt")
```
```{r echo=FALSE}
library(ggplot2)
library(ggrepel)
dfxy2 <- read.table("mds2.txt")
mx <- read.table("mx.txt")
ggplot(dfxy2, width=3,height=10, aes(x=x1, y=y1, label= rownames(mx), xlab="Coordinate 1", ylab="Coordinate 2", main="Metric MDS")
)+ geom_point(color = "red")+ geom_text_repel() + labs(title = "MDS Representation of 11 authors from Projecgt Gutenberg") +
  theme(axis.title.x=element_blank(),
    axis.text.x=element_blank(),
    axis.ticks.x=element_blank(),
    axis.title.y=element_blank(),
    axis.text.y=element_blank(),
    axis.ticks.y=element_blank())

```




```{r echo=FALSE}
library(huxtable)
library(magrittr)
mx_small <- mx[,1:9]
ht <- as_hux(mx_small, add_colnames = TRUE, add_rownames = TRUE)

bottom_border(ht)[1,]  <- 0.4
left_border(ht)[,2]    <- 0.4
align(ht)[,2]          <- 'right'
right_padding(ht)      <- 10
left_padding(ht)       <- 10
width(ht)              <- 1
number_format(ht)      <- 3

ht %>% 
      set_all_padding(4) %>% 
      set_outer_padding(0) %>% 
      set_number_format(2) %>% 
      set_width(0.4)
```

To further quantify the stylistic difference in a more rigorous manner, I have used the Kullback-Leibler divergence (DKL), as Hughes et al. have, which measures the distance between two distributions, much like a Euclidean distance between two points in many dimensions. The formula for computing DKL is as follows: 

$$D_{KL}(P_i,P_j)= \frac{1}{2}\sum\nolimits_{w \in \Omega}(P_i(w)log\frac{P_i(w)}{P_j(w)})+(P_j(w)log\frac{P_j(w)}{P_i(w)})$$
```{r echo=FALSE}
str = "a about above across after afterwards again against all almost alone along already also although always am among amongst amoungst amount an and another any anyhow anyone anything anyway anywhere are around as at back be became because become becomes becoming been before beforehand behind being below beside besides between beyond both bottom but by call can cannot cant con could couldnt cry describe detail do done down due during each eight either eleven else elsewhere empty enough etc even ever every everyone everything everywhere except few fifteen fify fill find fire first five for former formerly forty found four from front full further get give go had has hasnt have he hence her here hereafter hereby herein hereupon hers herself him himself his how however hundred ie if in inc indeed into is it its itself keep last latter latterly least less ltd made many may me meanwhile might mine more moreover most mostly move much must my myself name namely neither never nevertheless next nine no nobody none noone nor not nothing now nowhere of off often on once one only onto or other others otherwise our ours ourselves out over own part per perhaps please put rather re same see seem seemed seeming seems serious several she should show side since six sixty so some somehow someone something sometime sometimes somewhere still such take ten than that the their them themselves then thence there thereafter thereby therefore therein thereupon these they thin third this those though three through throughout thru thus to together too top toward towards twelve twenty two under until up upon us very via was we well were what whatever when whence whenever where whereafter whereas whereby wherein whereupon wherever whether which while whither who whoever whole whom whose why will with within without would yet you your yours yourself yourselves"
c <- strsplit(str," ")
# dfc <- data.frame(c[[1]])
# library(sprof)
# dfm <- list.as.matrix(dfc,byrow=FALSE,filler=NA)
# library(dplyr)
# library(huxtable)
# cht <- as_hux(dfm, add_colnames = FALSE, add_rownames = FALSE)
# cht %>%
#         set_all_padding(4) %>% 
#       set_outer_padding(0) %>% 
#       set_number_format(2) %>% 
#       set_caption("content-free words")

```

```{r echo=FALSE, eval=FALSE}
DKL_Distance <- function(tidy_text1, tidy_text2, cf_words) {
  cf_split <- strsplit(cf_words," ")
  cf <- c(1:length(cf_split[[1]]))  
  t1 <- c(1:length(cf_split[[1]]))
  t2 <- c(1:length(cf_split[[1]]))

  
  for (i in 1:length(cf_split[[1]])){
    key=toString(cf_split[[1]][i])
    cf[i] <- key
  }
  
  for (i in 1:length(cf)){
    val = cf[i]
    x = tidy_text1$word %>% 
        str_extract_all(val) %>%
        unlist %>%
        length
    t1[i]=x
    y = tidy_text2$word %>% 
        str_extract_all(val) %>%
        unlist %>%
        length
    t2[i]=y
  }
  t1 = t1/sum(t1)
  k1r <- data.frame(cf,t1)
  t2 = t2/sum(t2)
  k2r <- data.frame(cf,t2)
  sum = 0
  for (i in 1:length(cf)){
    if(k1r$t1[i]!=0 && k2r$t2[i]!=0){
      sum = sum + k1r$t1[i]*log(k1r$t1[i]/k2r$t2[i]) + k2r$t2[i]*log(k2r$t2[i]/k1r$t1[i])
    }
  }
  DKL = 0.5 * sum
  return(DKL)
}

DKL_hg_bronte <- DKL_Distance(tidy_hgwells,tidy_bronte,str)
write.table(DKL_hg_bronte, file="DKL_hg_bronte.txt")
```
```{r echo=FALSE}
DKL_hg_bronte  <- read.table("DKL_hg_bronte.txt")
```

The distance, in the case of H.G. Wells and the Bronte sisters (see above), is **0.0448**. We can use this method to compute the distances between many different pairs of authors. In our case, we'll use the set of translated fiction and the set of non-translated fiction, and compute the two sets of distances, in order to examine the effect of translation on style. 




```{r echo=FALSE, eval=FALSE}
library(rvest)
library(stringr)

ebooks_href <- c()
ebooks_text <- c()
for (abc in c("a","b","c","d","e","f","g","h","i","j","k","l","m","n","o","p","q","r","s","t","u","v","w","x","y","z")){
  url = paste("https://www.gutenberg.org/browse/authors/",abc,".html.utf8", sep="")
  ebooks_href <- rbind(ebooks_href, read_html(url) %>%
      html_nodes(".pgdbetext > a") %>%
      html_attr("href"))
  ebooks_text <- rbind(ebooks_text, read_html(url) %>%
      html_nodes(".pgdbetext") %>%
      html_text())
}

str_to_match = "(English)"
aa <- regexpr(str_to_match, ebooks_text)
str_to_match = "Translator)"
bb <- regexpr(str_to_match, ebooks_text)
tr_tl <- c()
ntr_tl<- c()
for (i in 1: length(ebooks_text)){
  if (aa[i] > -1 && bb[i] > -1){
    tr_tl <- rbind(tr_tl, ebooks_href[i])
  } else if (aa[i] > -1){
    ntr_tl <- rbind(ntr_tl, ebooks_href[i])
  }
}
tr_tl = str_replace(tr_tl, "/ebooks/","")
tr_tl = as.numeric(tr_tl)
ntr_tl = str_replace(ntr_tl, "/ebooks/","")
ntr_tl = as.numeric(ntr_tl)
```
```{r echo=FALSE, include=FALSE, eval=FALSE}
# get a list of fictions
gutenberg_subjects %>%
filter(subject_type == "lcsh") %>%
count(subject, sort = TRUE)
fictions <- gutenberg_subjects %>%
filter(str_detect(subject, "Fiction"))
fic <- distinct(fictions,gutenberg_id)
```
```{r echo=FALSE, eval=FALSE}
# only keep fictions
tr_tl <- tr_tl[tr_tl %in% fic$gutenberg_id]
ntr_tl <- ntr_tl[ntr_tl %in% fic$gutenberg_id]

# error handling
x=0
for (i in 1:length(tr_tl)){
  if (tr_tl[i]==689) {x=i} 
}
if (x>0) {tr_tl <- tr_tl[-x]}
x=0
for (i in 1:length(tr_tl)){
 if (tr_tl[i]==8993) {x=i}
}
if (x>0) {tr_tl <- tr_tl[-x]}
x=0
for (i in 1:length(ntr_tl)){
 if (ntr_tl[i]==4468) {x=i}
}
if (x>0) {ntr_tl <- ntr_tl[-x]}
x=0
for (i in 1:length(ntr_tl)){
 if (ntr_tl[i]==4484) {x=i}
}
if (x>0) {ntr_tl <- ntr_tl[-x]}
for (i in 1:length(ntr_tl)){
 if (ntr_tl[i]==4492) {x=i}
}
if (x>0) {ntr_tl <- ntr_tl[-x]}

length_tr_tl <- length(tr_tl)
length_ntr_tl <- length(ntr_tl)
```

## Analysis

In order to compare whether there is a significant stylistic difference between translated fiction and non-translated fiction, I've drawn two sets of 100 random pairs. For the first set, the pairing is between a translated work and a non-translated work; for the second set, the pairing is between two non-translated works. I've examined whether there is a significant difference between those sets of distances. For each pair, I've computed the Kullback-Leibler distance and performed a logarithmic transformation. dists_ntr_tr indicates the distribution of distances between a non-translated work and a translated work; dists_ntr_ntr indicates the distribution of distances between two non-translated works.

```{r echo=FALSE, eval=FALSE}
n = 100

dists_ntr_ntr = rep(0,n)
for (i in 1:n){
  t <- sample(ntr_tl,2,replace=FALSE)
  text_ID <- str_split(t, " ")
  t1 <- gutenberg_download(text_ID[[1]])
  t2 <- gutenberg_download(text_ID[[2]])
  tt1 <- t1 %>%
  group_by(gutenberg_id) %>%
  mutate(linenumber = row_number()) %>%
  ungroup() %>%
  unnest_tokens(word, text)
  tt2 <- t2 %>%
  group_by(gutenberg_id) %>%
  mutate(linenumber = row_number()) %>%
  ungroup() %>%
  unnest_tokens(word, text) 
  dists_ntr_ntr[i] <- DKL_Distance(tt1,tt2,str)
}

dists_ntr_tr = rep(0,n)
for (i in 1:n){
  t1 <- gutenberg_download(sample(ntr_tl,1))
  t2 <- gutenberg_download(sample(tr_tl,1))
  tt1 <- t1 %>%
  group_by(gutenberg_id) %>%
  mutate(linenumber = row_number()) %>%
  ungroup() %>%
  unnest_tokens(word, text)
  tt2 <- t2 %>%
  group_by(gutenberg_id) %>%
  mutate(linenumber = row_number()) %>%
  ungroup() %>%
  unnest_tokens(word, text) 
  dists_ntr_tr[i] <- DKL_Distance(tt1,tt2,str)
}

# dists_tr_tr = rep(0,n)
# for (i in 1:n){
#   t <- sample(tr_tl,2,replace=FALSE)
#   text_ID <- str_split(t, " ")
#   t1 <- gutenberg_download(text_ID[[1]])
#   t2 <- gutenberg_download(text_ID[[2]])
#   tt1 <- t1 %>%
#   group_by(gutenberg_id) %>%
#   mutate(linenumber = row_number()) %>%
#   ungroup() %>%
#   unnest_tokens(word, text)
#   tt2 <- t2 %>%
#   group_by(gutenberg_id) %>%
#   mutate(linenumber = row_number()) %>%
#   ungroup() %>%
#   unnest_tokens(word, text) 
#   dists_tr_tr[i] <- DKL_Distance(tt1,tt2,str)
# }

mean(dists_ntr_ntr)
mean(dists_ntr_tr)

library(reshape2)
trials <- c(1:n)
df1 <- data.frame(log(dists_ntr_tr), log(dists_ntr_ntr), trials)
write.table(df1,"mydata2.txt")



########################
# do ntr_tr vs ntr_ntr #
########################





```

```{r echo=FALSE}
library(reshape2)
library(lattice)
n=100
trials <- c(1:n)
df1 <- read.table("mydata2.txt")
df2 <- melt(df1, id.vars='trials')
df2 <- subset(df2,variable!="log.dists_tr_tr.")
ggplot(df2, aes(x=trials, y=value, fill=variable)) + 
  geom_boxplot() +    
  theme(axis.title.x=element_blank(),
    axis.text.x=element_blank(),
    axis.ticks.x=element_blank())
#ggplot(df2, aes(x=trials, y=value, fill=variable)) + 
#  geom_boxplot(ylim=c(0,0.2)) + coord_cartesian(ylim = c(0, 0.19)) +
#  theme(axis.title.x=element_blank(),
#    axis.text.x=element_blank(),
#    axis.ticks.x=element_blank())
```
```{r echo=FALSE}
qqmath(~value|variable,data=df2,col=rgb(1,0,0,.1),
prepanel = prepanel.qqmathline,
       panel = function(x, ...) {
          panel.qqmathline(x, ...)
          panel.qqmath(x, ...)
       }
)


#df1$log.dists_ntr_ntr. = (df1$log.dists_ntr_ntr. - mean(df1$log.dists_ntr_ntr.)) / (sd(df1$log.dists_ntr_ntr.)/sqrt(n))
#df1$log.dists_tr_tr. = (df1$log.dists_tr_tr. - mean(df1$log.dists_tr_tr.)) / (sd(df1$log.dists_tr_tr.)/sqrt(n))
#df1$log.dists_ntr_tr. = (df1$log.dists_ntr_tr. - mean(df1$log.dists_ntr_tr.)) / (sd(df1$log.dists_ntr_tr.)/sqrt(n))
#qqnorm(df1$log.dists_ntr_tr., main="QQPLOT between a non-translated work and a translated work", col = rgb(1,0,0,.2)) 
#qqline(df1$log.dists_ntr_tr., col = "blue")
#qqnorm(df1$log.dists_ntr_ntr., main="QQPLOT between two non-translated works", col = rgb(1,0,0,.2))
#qqline(df1$log.dists_ntr_ntr., col = "blue")
```
The QQPLOTs seem roughly linear, though not perfectly linear, and I'll assume that the log of the KL Distances approximately follows a normal distribution. I will not assume that the two sets of distances have the same variance. Judging from the box plots, the variances are similar enough that I should be able to use the two-sample t-test.
```{r echo=FALSE}
library(ggpubr)
t.test(df1$log.dists_ntr_tr., df1$log.dists_ntr_ntr., alternative = "two.sided", var.equal = FALSE)
# from this we can say there's not enough evidence to reject the null, meaning there's not enough evidence to say they're the same, mean
library(dplyr)
df2 <- df2[!(df2$variable=="log.dists_tr_tr."),]
df2 <- df2 %>%
  mutate(binary = case_when(df2$variable == "log.dists_ntr_tr." ~ 1,
                                df2$variable == "log.dists_ntr_ntr." ~ 0))
dummy_model <- lm(binary~value,data=df2)
summary(dummy_model)
```
<br/>
Shown above are two tests. First, a two-sample t-test whose null hypothesis is that the two sets of distances have the same mean. In other words the test assumes equivalence and looks for evidence to the contrary. What we find here is a high p-value of **0.3877**, well above the typical 0.05 threshold, meaning there is not enough evidence to suggest that the means are significantly different. In other words, the observed data does not indicate that the translated works are stylistically different from non-translated works. The test, by nature, also does not allow us to claim that they are significantly similar.

Second test shown above is a regression test that tests whether the type of pairing (translated-nontranslated or nontranslated-nontranslated) is correlated with the distance. We observe that the R-squared values are very small, suggesting there is hardly any correlation between the type of pairing and the stylistic distance.

In order to then test for equivalence, I used the TOST (Two one-sided tests) method, setting the interval of equivalence to +/- 0.25%, as shown below.

```{r echo=FALSE}
differences <-c(1:10000)
count=1
for (i in 1:100){
  for (j in 1:100){
    differences[count] = df1$log.dists_ntr_tr.[i]-df1$log.dists_ntr_ntr.[j]
    count=count+1
  }
}
mean_diff = mean(differences)
library(equivalence)
tost(df1$log.dists_ntr_tr., df1$log.dists_ntr_ntr., epsilon = 0.25, paired = FALSE)
```

<br/>
The average difference between the two groups is 0.0466, and an epsilon of 0.25% examines a window as small as +/-0.000129. Since the p-value of **0.000104** is much smaller than the conventional 0.05 benchmark, we are able to conclude that the two groups are effectively equivalent, meaning the distance between a translated work and a nontranslated work is effectively equivalent to the distance between a nontranslated work and another nontranslated work. This implies that there is no significant stylistic difference between translated works and non-translated works.


## Conclusion

The study finds that there is no significant stylistic difference between translated works and non-translated works, as observed by the distribution of content-free words. Any readerly claim to be able to recognize a translated text as such, or that a certain short story "sounds translated," appears to be unjustifiable. 

One of the assumptions that could be further examined is whether the logarithms of Kullback-Leibler distances are indeed normally distributed. The tail ends of the QQPLOTs appear to be nonlinear. Another assumption that should be studied is the specific set of content-free words. It is a specific variation on Cyril Labbe's method for calculating "intertextual distance," which considers every word in all the texts, not just the specific set of content-free words that have been subjectively determined. Of course, there is also the question of whether we can meaningfully associate content-free words and the bag-of-words approach to an author's style.

One may expand upon this study in terms of stratifying the data according to temporal, geographical, and linguistic adjacency. Hughes et al. have found that there is greater similarity between the styles of similar time periods, for example, so it would be worth examining whether there is significant stylistic difference between a translated text and a nontranslated text in a similar time period. It would also be great to expand the study beyond the texts available in Project Gutenberg. The organizers of Gutenberg appear to have decided there is no need for more than one translation of the same original text. An interesting study could be conducted on multiple translations of a single author-text, or across all the translations by the same translator. Even to study this latter case, Project Gutenberg has too little data on translated works.

There are many possibilities for interpreting the result of this studies. One can conjucture, for example, that what makes a Russian literature sound Russian is not through the style as characterized by the translator's use of prepositions, pronouns, and so on, that translation does not communicate culture through such a style. Perhaps what tips off a reader that a certain text has been translated are the content words, rather than the content-free words. There is no doubt that the themes and motifs that recur in Russian literature remind us of its cultural backdrop, so one might be tempted to conclude that the Russian-ness has more to do with meaning-filled nouns and verbs. However, one cannot ignore the fact that different translations sound different from one another, and that there are schools that manifestly aim to render their translations sound "more natural" -- more like a non-translated work. What is it, then, that makes something sound more like a non-translated work? How can we characterize this aspect of style? How do we tease apart the original author's contribution and the translator's contribution to the style of the translated text? I hope these questions can be addressed in further studies.


## Appendix
The list of the content-free words used:
```
a about above across after afterwards again against all almost alone along already also although always am among amongst amoungst amount an and another any anyhow anyone anything anyway anywhere are around as at back be became because become becomes becoming been before beforehand behind being below beside besides between beyond both bottom but by call can cannot cant con could couldnt cry describe detail do done down due during each eight either eleven else elsewhere empty enough etc even ever every everyone everything everywhere except few fifteen fify fill find fire first five for former formerly forty found four from front full further get give go had has hasnt have he hence her here hereafter hereby herein hereupon hers herself him himself his how however hundred ie if in inc indeed into is it its itself keep last latter latterly least less ltd made many may me meanwhile might mine more moreover most mostly move much must my myself name namely neither never nevertheless next nine no nobody none noone nor not nothing now nowhere of off often on once one only onto or other others otherwise our ours ourselves out over own part per perhaps please put rather re same see seem seemed seeming seems serious several she should show side since six sixty so some somehow someone something sometime sometimes somewhere still such take ten than that the their them themselves then thence there thereafter thereby therefore therein thereupon these they thin third this those though three through throughout thru thus to together too top toward towards twelve twenty two under until up upon us very via was we well were what whatever when whence whenever where whereafter whereas whereby wherein whereupon wherever whether which while whither who whoever whole whom whose why will with within without would yet you your yours yourself yourselves
```